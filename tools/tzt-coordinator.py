# SPDX-License-Identifier: LGPL-3.0-or-later
# tzt-coordinator.py -- coordinator server for time zone tiling
#
# Copyright (c) 2022 Alex Harsányi <AlexHarsanyi@gmail.com>
#
# This program is free software: you can redistribute it and/or modify it
# under the terms of the GNU Lesser General Public License as published by
# the Free Software Foundation, either version 3 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public
# License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

import socketserver, socket, json, sqlite3, os, argparse;
from os import path;
from io import BytesIO, StringIO;
from datetime import datetime, timedelta;


# ............................................................. logging ....

def log(msg):
    # The log function puts a timestamp as the number of seconds since the
    # program start -- this is more useful for this type of program than a
    # timestamp.
    duration = (datetime.now() - log.start_time).total_seconds();
    msg = (f"[{duration:>8.1f}] {msg}");
    if log.to_stdout:
        print(msg);
    if log.file_output != None:
        log.file_output.write(msg);
        log.file_output.write('\n');

log.to_stdout = True;            # whether to log to stdout
log.file_output = None;          # file to log output to (initialized later)
log.start_time = datetime.now(); # tell the log function when the program started


# ..................................................... min_geoid_level ....

# Geoid level 24 to refinement level mapping (See `min_geoid_level`): List is
# generated by running the tests in the tzgeolookup package and listing geoids
# for test cases that map to multiple time zones.  We try to put in a geiod
# refinement level which would make the test pass, i.e assign the correct
# sub-geoids to the right time zone.

geoid_min_level = {
    0xd41000000000000 : 13,  0x783000000000000 : 13,  0xe37000000000000 : 13, 0xe2b000000000000 : 13,
    0x7af000000000000 : 13,  0xe55000000000000 : 13,  0x7a5000000000000 : 13, 0x1c01000000000000 : 13,
    0x14ed000000000000 : 13, 0x1985000000000000 : 13, 0x1745000000000000 : 13,0x1a85000000000000 : 13,
    0x1349000000000000 : 13, 0x1149000000000000 : 13, 0x194d000000000000 : 13, 0x190d000000000000 : 13,
    0x1e99000000000000 : 13, 0x1999000000000000 : 13, 0x191d000000000000 : 13, 0x11dd000000000000 : 13,
    0x105d000000000000 : 13, 0x109d000000000000 : 13, 0x1121000000000000 : 13, 0x1361000000000000 : 13,
    0x1bb9000000000000 : 13, 0x1179000000000000 : 13, 0x1183000000000000 : 13, 0x1107000000000000 : 13,
    0x1a0f000000000000 : 13, 0x1917000000000000 : 13, 0x1773000000000000 : 13, 0x1a77000000000000 : 13,
    0x19fb000000000000 : 13, 0x16ff000000000000 : 13, 0x174b000000000000 : 13, 0x104b000000000000 : 13,
    0x11d3000000000000 : 13, 0x1713000000000000 : 13, 0x19a7000000000000 : 13, 0x1a67000000000000 : 13,
    0x2c83000000000000 : 13, 0x2afb000000000000 : 13, 0x3909000000000000 : 13, 0x374d000000000000 : 13,
    0x3ed1000000000000 : 13, 0x3899000000000000 : 13, 0x3e69000000000000 : 13, 0x3c29000000000000 : 13,
    0x30e9000000000000 : 13, 0x390b000000000000 : 13, 0x314f000000000000 : 13, 0x3893000000000000 : 13,
    0x30db000000000000 : 13, 0x3bdf000000000000 : 13, 0x31f3000000000000 : 13, 0x3737000000000000 : 13,
    0x327f000000000000 : 13, 0x3583000000000000 : 13, 0x3fc3000000000000 : 13, 0x4cc1000000000000 : 13,
    0x4795000000000000 : 13, 0x4fd9000000000000 : 13, 0x4fdd000000000000 : 13, 0x4fe9000000000000 : 13,
    0x47ad000000000000 : 13, 0x4875000000000000 : 13, 0x433d000000000000 : 13, 0x45c5000000000000 : 13,
    0x46c5000000000000 : 13, 0x4709000000000000 : 13, 0x4889000000000000 : 13, 0x434d000000000000 : 13,
    0x45cd000000000000 : 13, 0x488d000000000000 : 13, 0x4231000000000000 : 13, 0x40f9000000000000 : 13,
    0x45b9000000000000 : 13, 0x45cb000000000000 : 13, 0x4fdf000000000000 : 13, 0x44ef000000000000 : 13,
    0x4e03000000000000 : 13, 0x4fc3000000000000 : 13, 0x4887000000000000 : 13, 0x4407000000000000 : 13,
    0x4fc7000000000000 : 13, 0x42c7000000000000 : 13, 0x4687000000000000 : 13, 0x470f000000000000 : 13,
    0x4e0f000000000000 : 13, 0x414f000000000000 : 13, 0x488f000000000000 : 13, 0x40d3000000000000 : 13,
    0x4d53000000000000 : 13, 0x4d57000000000000 : 13, 0x40d7000000000000 : 13, 0x4357000000000000 : 13,
    0x46db000000000000 : 13, 0x44db000000000000 : 13, 0x4c5b000000000000 : 13, 0x45db000000000000 : 13,
    0x47a7000000000000 : 13, 0x4e27000000000000 : 13, 0x4e2b000000000000 : 13, 0x442b000000000000 : 13,
    0x45eb000000000000 : 13, 0x47f3000000000000 : 13, 0x43f3000000000000 : 13, 0x4137000000000000 : 13,
    0x4237000000000000 : 13, 0x46fb000000000000 : 13, 0x467b000000000000 : 13, 0x43fb000000000000 : 13,
    0x447b000000000000 : 13, 0x467f000000000000 : 13, 0x423f000000000000 : 13, 0x5cc5000000000000 : 13,
    0x5249000000000000 : 13, 0x50cd000000000000 : 13, 0x51d1000000000000 : 13, 0x5c95000000000000 : 13,
    0x5725000000000000 : 13, 0x5bad000000000000 : 13, 0x5db9000000000000 : 13, 0x5b3d000000000000 : 13,
    0x5a19000000000000 : 13, 0x5b19000000000000 : 13, 0x5cd9000000000000 : 13, 0x535d000000000000 : 13,
    0x5e9d000000000000 : 13, 0x52a9000000000000 : 13, 0x5d69000000000000 : 13, 0x5da9000000000000 : 13,
    0x51b1000000000000 : 13, 0x5bb1000000000000 : 13, 0x51b5000000000000 : 13, 0x5275000000000000 : 13,
    0x5103000000000000 : 13, 0x598b000000000000 : 13, 0x5a3f000000000000 : 13, 0x53d7000000000000 : 13,
    0x5a57000000000000 : 13, 0x5e9b000000000000 : 13, 0x5ddb000000000000 : 13, 0x515b000000000000 : 13,
    0x51df000000000000 : 13, 0x511f000000000000 : 13, 0x5d23000000000000 : 13, 0x5a23000000000000 : 13,
    0x5167000000000000 : 13, 0x5ba7000000000000 : 13, 0x536b000000000000 : 13, 0x5dab000000000000 : 13,
    0x51ef000000000000 : 13, 0x502f000000000000 : 13, 0x51b3000000000000 : 13, 0x5a33000000000000 : 13,
    0x59f7000000000000 : 13, 0x5037000000000000 : 13, 0x6a69000000000000 : 13, 0x6afb000000000000 : 13,
    0x8681000000000000 : 13, 0x8585000000000000 : 13, 0x880d000000000000 : 13, 0x8891000000000000 : 13,
    0x8fd5000000000000 : 13, 0x8f5d000000000000 : 13, 0x886d000000000000 : 13, 0x8871000000000000 : 13,
    0x8435000000000000 : 13, 0x8dfd000000000000 : 13, 0x8e59000000000000 : 13, 0x85d9000000000000 : 13,
    0x8e21000000000000 : 13, 0x8f61000000000000 : 13, 0x8721000000000000 : 13, 0x8825000000000000 : 13,
    0x8f25000000000000 : 13, 0x8f83000000000000 : 13, 0x871f000000000000 : 13, 0x8677000000000000 : 13,
    0x8817000000000000 : 13, 0x8f97000000000000 : 13, 0x8757000000000000 : 13, 0x8863000000000000 : 13,
    0x8f63000000000000 : 13, 0x8723000000000000 : 13, 0x8827000000000000 : 13, 0x8f67000000000000 : 13,
    0x8f3b000000000000 : 13, 0x88bb000000000000 : 13, 0x887b000000000000 : 13, 0x9699000000000000 : 13,
    0x916d000000000000 : 13, 0x9679000000000000 : 13, 0x96bd000000000000 : 13, 0x9481000000000000 : 13,
    0x9241000000000000 : 13, 0x9683000000000000 : 13, 0x944b000000000000 : 13, 0x91cf000000000000 : 13,
    0x9197000000000000 : 13, 0x919b000000000000 : 13, 0x965f000000000000 : 13, 0x926b000000000000 : 13,
    0x926f000000000000 : 13, 0x9377000000000000 : 13, 0x9447000000000000 : 13, 0x9207000000000000 : 13,
    0x967f000000000000 : 13, 0x923f000000000000 : 13, 0xaf05000000000000 : 13, 0xaf8d000000000000 : 13,
    0xaff7000000000000 : 13, 0xaf3b000000000000 : 13, 0xbac9000000000000 : 13, 0xb04d000000000000 : 13,
    0xb011000000000000 : 13, 0xb01d000000000000 : 13, 0xb1c5000000000000 : 13, 0xb005000000000000 : 13,
    0xbaa9000000000000 : 13, 0xb029000000000000 : 13, 0xb083000000000000 : 13, 0xb007000000000000 : 13,
    0xb00b000000000000 : 13, 0xb00f000000000000 : 13, 0xb06f000000000000 : 13, 0xb073000000000000 : 13,
    0xb077000000000000 : 13, 0xb01b000000000000 : 13, 0xbd9b000000000000 : 13
};

# Map time zone names to refinement level for each time zone (See
# `min_geoid_level`).  The list is empirically determined -- mostly countries
# which are surrounded by water, or have not many settlements along their
# boundaries. Note that we can refine specific GEOIDs for these by placing
# them in the `geoid_min_level` hash, so, even if there is a settlement along
# the border, we don't need to refine the entire time zone to a lower level.

tzname_min_level = {
    'Asia/Tokyo' : 17,
    'Australia/Perth' : 17,
    'Australia/Hobart' : 17,
    'America/Toronto' : 17,
    'America/Vancouver' : 17,
    'America/Santiago' : 17,
    'America/Sao_Paulo' : 17
};

# This function determines the minimum geoid level to which to refine a
# timezone or geoid.  This is a tradeoff process -- the higher the level, the
# more precise the refinement, but also the larger the resulting data set.
# Given that most of the world has no "interesting" locations along the time
# zone borders, we can use a more coarse refinement for most of the places and
# a finer refinement where it is needed.

def min_geoid_level(tzname, geoid):
    # ... to determine the minimum refinement level ...
    try:
        # ... check first if we have a geoid that we need to refine.  Note
        # that the GEOID parameter is a string, while we store them as numbers
        # in the hash table...
        return geoid_min_level[int(geoid)];
    except KeyError:
        pass;                           # tru the next one

    try:
        # ... than check if we have a specific time zone we want to refine...
        return tzname_min_level[tzname];
    except KeyError:
        pass;

    # Otherwise we refine by continents...
    if (tzname.startswith('Pacific/')
        or tzname.startswith('Indian/')
        or tzname.startswith('Atlantic/')
        or tzname.startswith('Antarctica/')
        or tzname.startswith('Arctic/')):
        return 17;                      # about 1600m avg error
    elif (tzname.startswith('Australia/')
        or tzname.startswith('America/')
        or tzname.startswith('Africa/')
        or tzname.startswith('Asia/')):
        return 16;                      # about 800m avg error
    else:
        # This leaves Europe last ...
        return 15;                      # about 400m avg error

# Utility function to print out a geoid as a hex value -- used for logging

def pgeoid(g):
    return '#x' +  hex(int(g));

# Make TZNAME suitable as a file or directory name by replacing invalid
# characters.  Timezones have the / character in their name, which is not
# allowed in a file name as it is a directory separator.

def sanitize(tzname):
    return tzname.replace("/", "+");

# Create the initial table and tasks for the coordinator database -- the
# initial tasks are a coarse tiling task for each time zone name from the
# TZ_DEFINITIONS json object.  FILE_NAME is the SQLite database to create.

def maybe_initialize_new_database(file_name, tz_definitions):
    if path.exists(file_name):
        log(f"Database file {file_name} already exists");
        return False;
    connection = sqlite3.connect(file_name);
    cursor = connection.cursor();
    ## NOTE: geoids are stored as text, since they don't fit into the database
    ## as full 64bit values.
    cursor.execute('''
    CREATE TABLE tasks(
    id INTEGER PRIMARY KEY,
    tzname TEXT NOT NULL,
    geoid text,
    client text,
    start_time integer,
    end_time integer)
    ''');
    for feature in tz_definitions['features']:
        properties = feature['properties'];
        tzid = properties['tzid'];
        if not tzid.lower().startswith('etc/'):
            cursor.execute('INSERT INTO tasks(tzname) VALUES(?)', (tzid,));
    connection.commit();
    connection.close();
    log(f"Created tasks database at {file_name}");
    return True;

# Write GEOIDS, a list of geoids into the FILE_NAME.  The "file format" is one
# item per line, with the first item being the number of geoids, followed by
# the geoids themselves.
#
# We write to a temporary file and move it -- writing might take a long time,
# and there is a bigger chance of failing mid-way, corrupting the GEOID file.
# With rename, the only window of opportunity for failure is when the file is
# renamed, which is also an atomic filesystem operation.

def write_geoids(geoids, file_name):
    tmp = file_name + ".tmp";
    with open(tmp, 'w') as o:
        o.write(f"{len(geoids)}\n");
        for geoid in geoids:
            o.write(f"{geoid}\n");
    os.rename(tmp, file_name);

# Add some "geoid refine" tasks for the time zone TZNAME to the database from
# the list of GEOIDS

def put_refine_tasks(dbc, tzname, geoids):
    for geoid in geoids:
        dbc.execute('INSERT INTO tasks(tzname, geoid) VALUES(?, ?)',
                    (tzname, 'G' + geoid));

# Assign the task TASK_ID to CLIENT_ID and return the task itself to be sent
# to the client.

def do_assign_task(dbc, client_id, task_id, tzname, geoid):
    dbc.execute(
        'UPDATE tasks SET client = ?, start_time = ? WHERE id = ?',
        (client_id, datetime.today().timestamp(), task_id));
    if geoid != None:
        geoid = geoid[1:];          # Strip off the G
    return ("new_task", tzname, geoid);

# Find the maximum time it took to process a request, and use that to
# determine if we have overdue tasks.

def find_max_task_duration(dbc):
    dbc.execute(
        'SELECT max(end_time - start_time) from tasks');
    row = dbc.fetchone();
    if row[0] == None:
        # No completed tasks in the database, return an arbitrary, but large
        # value (15 minutes)
        return 900;
    return row[0];

# Try to assign a task to CLIENT_ID, see comments in implementation on how
# this works.

def assign_task(dbc, client_id):

    # First, try assigning a free task from the database.  A free task is one
    # that does not have a start time.
    dbc.execute(
        'SELECT max(id), tzname, geoid FROM tasks WHERE start_time IS NULL');
    row = dbc.fetchone();

    # NOTE: the query above should always return a row, but max(id) might be
    # null.  Still, it is good practice to check for no row as well...
    if row != None and row[0] != None:
        return do_assign_task(dbc, client_id, row[0], row[1], row[2]);

    # Next, try to find an old but incomplete task and maybe re-assign that
    dbc.execute('''
    SELECT id, tzname, geoid, start_time
    FROM tasks
    WHERE start_time IS NOT NULL and end_time IS NULL
    ORDER BY start_time''');
    row = dbc.fetchone();

    if row == None:
        # All task are complete, we're done
        return ("complete", None, None);

    start_time = row[3];                # timestamp when the old task started;
    overdue = 2.0 * find_max_task_duration(dbc);

    if (datetime.today().timestamp() - start_time) > overdue:
        # This task has been assigned a long time ago, maybe the client
        # abandoned it, so we re-assign it.
        return do_assign_task(dbc, client_id, row[0], row[1], row[2]);

    # We could not find a suitable task, but there are still outstanding tasks
    # in the database.  It is worth trying again later.
    return ("retry", None, None);

# This is the request handled object for the server.  One of these is
# instantiated for each connection and handles requests and replies from the
# client.

class TztRequestHandler(socketserver.BaseRequestHandler):

    def log(self, msg):
        log(f"{self.client_address}: {msg}");

    def send_reply(self, reply):
        sdata = StringIO();             # json only dumps to strings
        json.dump(reply, sdata);
        data = bytes(sdata.getvalue(), encoding='utf-8');
        msize = len(data).to_bytes(byteorder='big', length=4);
        self.request.sendall(msize);
        self.request.sendall(data);

    def mark_complete(self, task_id):
        self.server.cursor.execute(
            'UPDATE tasks SET end_time = ? WHERE id = ?',
            (datetime.today().timestamp(), task_id));

    def on_request_tzdef(self, tzname):
        self.log(f"Requested TimeZone definition for {tzname}")
        try:
            # Try to send an already serialized timezone definition from the
            # cache.  Some definitions are large, and if we have 24 - 32
            # workers all requesting the same definition, we can save some
            # considerable time...
            data = self.server.serialized_tzdata_cache[tzname];
            msize = len(data).to_bytes(byteorder='big', length=4);
            self.request.sendall(msize);
            self.request.sendall(data);
            self.log(f"Sent TimeZone definition for {tzname}")
        except KeyError:
            result = None;
            for feature in server.tzdata['features']:
                if feature['properties']['tzid'] == tzname:
                    result = feature;
                    break;

            if result == None:
                self.send_reply(
                    {'result' : 'error',
                     'message' : "%s not found" % (tzname,)});
                self.log(f"TimeZone definition for {tzname} not found");
                return;

            sdata = StringIO();             # json only dumps to strings
            json.dump(result, sdata);
            data = bytes(sdata.getvalue(), encoding='utf-8');
            msize = len(data).to_bytes(byteorder='big', length=4);
            self.request.sendall(msize);
            self.request.sendall(data);
            self.server.serialized_tzdata_cache[tzname] = data;
            self.log(f"Sent TimeZone definition for {tzname}");

    def on_request_task(self, client_id):
        # NOTE: we select the max task id, to put a last-in first-out
        # strategy, this is so that we don't force our clients to create
        # regions for all timezones, since the coarse tiling tasks would be
        # first.  This way, we hope that the first few clients would
        # coarse-tile the timezones than we'll start refining geoids in those
        # same timezones.
        (task, tzname, geoid) = assign_task(self.server.cursor, client_id);
        if task == "complete":
            self.log("No pending tasks in the database (all tasks completed)");
            self.send_reply({'task':'complete'});
        elif task == "retry":
            self.log("No free tasks, waiting for some in-progress tasks");
            self.send_reply({'task':'retry'});
        elif geoid == None:
            self.log(f"Assigned coarse tiling for {tzname}");
            self.send_reply({'task':'coarse_tiling',
                             'tzname': tzname,
                             'level' : self.server.max_geoid_level});
        else:
            self.log(f"Assigned refining for {tzname} {pgeoid(geoid)}");
            self.send_reply({'task':'refine_tiling',
                             'tzname': tzname,
                             'geoid' : geoid,
                             'level' : min_geoid_level(tzname, geoid)});

    def on_coarse_tiling_complete(self, reply):
        try:
            tzname = reply['tzname'];
            client_id = reply['client_id'];
            # Check if this client was in fact assigned this refine-tiling
            # task.
            self.server.cursor.execute(
                'SELECT id FROM tasks WHERE tzname = ? AND client = ?',
                (tzname, client_id));
            row = self.server.cursor.fetchone();
            if row == None:
                self.send_reply({'result':'error'});
                self.log("on_coarse_tiling_complete - unknown task");
            else:
                self.log(f"Received coarse tiling for {tzname}, contains {len(reply['contains'])}, intersects {len(reply['intersects'])}")
                task_id = row[0];
                # Write out the contained and intersects geoids and add new
                # tasks for the intersects geoids.  Also delete this task from
                # the database.
                p = os.path.join(self.server.output_dir, sanitize(tzname));
                os.makedirs(p, exist_ok = True);
                write_geoids(reply['contains'], os.path.join(p, 'contains.dat'));
                write_geoids(reply['intersects'], os.path.join(p, 'intersects.dat'));
                put_refine_tasks(self.server.cursor, tzname, reply['intersects']);
                self.mark_complete(task_id);
                self.send_reply({'result':'ok'});
        except KeyError:
            self.send_reply({'result':'error'});
            self.log("bad request");

    def on_refine_tiling_complete(self, reply):
        try:
            tzname = reply['tzname'];
            geoid = reply['geoid'];
            client_id = reply['client_id'];
            # Check if this client was in fact assigned this refine-tiling
            # task.
            self.server.cursor.execute(
                'SELECT id FROM tasks WHERE tzname = ? AND geoid = ? AND client = ?',
                (tzname, 'G' + geoid, client_id));
            row = self.server.cursor.fetchone();
            if row == None:
                self.send_reply({'result':'error'});
                self.log("on_refine_tiling_complete - unknown task");
            else:
                self.log(f"Received refined tiling for {tzname} geoid {pgeoid(geoid)}, {len(reply['refinement'])} geoids");
                task_id = row[0];
                # Write out the contained and intersects geoids and add new
                # tasks for the intersects geoids.  Also delete this task from
                # the database.
                p = os.path.join(self.server.output_dir, sanitize(tzname));
                os.makedirs(p, exist_ok = True);
                write_geoids(reply['refinement'], os.path.join(p, '%s.dat' % (geoid,)));
                self.mark_complete(task_id);
                self.send_reply({'result':'ok'});
        except KeyError:
            self.send_reply({'result':'error'});
            self.log("bad request");

    # Note: this will handle one message, than the connection will be closed
    # and the client will have to re-connect again
    def do_handle(self):
        msize = int.from_bytes(self.request.recv(4), byteorder='big');
        mdata = bytearray();
        while msize > 0:
            mchunk = self.request.recv(msize);
            if not mchunk or mchunk == b'':
                self.log("closed connection");
                return;
            mdata += bytearray(mchunk);
            msize = msize - len(mchunk);
        io = BytesIO(mdata);
        message = json.load(io);

        try:
            if message['key'] != self.server.application_key:
                self.log("bad application key");
                return;
        except KeyError:
            self.log("no application key");
            return;

        try:
            op = message['operation'];
            if op == 'request_task':
                self.on_request_task(message['client_id']);
            elif op == 'request_tzdata':
                self.on_request_tzdef(message['tzname']);
            elif op == 'coarse_tiling':
                self.on_coarse_tiling_complete(message);
            elif op == 'refine_tiling':
                self.on_refine_tiling_complete(message);
            else:
                self.log(f"bad operation: {op}");
        except KeyError:
            self.log(f"bad message: {message}");

    def handle(self):
        try:
            self.do_handle();
            if self.server.db.in_transaction:
                self.server.db.commit();
        except Exception as e:
            self.log(f"Caught {e}");
            if self.server.db.in_transaction:
                self.server.db.rollback();
        except:
            self.log("unknown exception caught");
            if self.server.db.in_transaction:
                self.server.db.rollback();

# The server class is just a TCPServer, but we allow reuse of addresses and
# increase the queue size so we can process a larger number of client workers.

class TztTCPServer(socketserver.TCPServer):
    allow_reuse_address = True;
    request_queue_size = 128;

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='''
    Hand out tiling tasks to clients and store tiling results''');
    parser.add_argument(
        '-p', '--port',
        type=int,
        default=2395,
        help='port for the TCP service');
    parser.add_argument(
        '-o', '--output',
        default="./output",
        help='output dir for data and logs');
    parser.add_argument(
        '-i', '--input',
        default="./combined.json",
        help='time zone GeoJSON data file');
    parser.add_argument(
        '-q', '--quiet',
        default=False,
        action='store_true',
        help="don't print messages to stdout");
    parser.add_argument(
        '-c', '--clear',
        default=False,
        action='store_true',
        dest='clear_pending_tasks',
        help="clear (reset) pending tasks");

    args = parser.parse_args();

    log.to_stdout = not args.quiet;
    os.makedirs(args.output, exist_ok = True);
    db_file_name = os.path.join(args.output, "tzt-coordinator.db");
    log_file_name = os.path.join(args.output, "tzt-coordinator.log");
    tz_data_file = args.input;

    log.file_output = open(log_file_name, mode='at', buffering=1);
    log(f"Program started at {log.start_time}");

    log(f"Loading TimeZone data from {tz_data_file} ...");
    with open(tz_data_file) as inp:
        tzdata = json.load(inp);
    log(f"Loading TimeZone data from {tz_data_file} ... done.");
    maybe_initialize_new_database(db_file_name, tzdata);
    log(f"Listening on port {args.port}");

    with TztTCPServer(("", args.port), TztRequestHandler) as server:
        server.db = sqlite3.connect(db_file_name);
        server.cursor = server.db.cursor();
        server.application_key = 'tzt-8';
        server.max_geoid_level = 24;
        server.min_geoid_level = 15;
        server.output_dir = args.output;
        server.tzdata = tzdata;
        server.serialized_tzdata_cache = {};

        # Reset any tasks that were handed out and were not completed (these
        # will be handed out again to new clients when they connect)
        if args.clear_pending_tasks:
            log("Clearing pending tasks from the database...");
            server.cursor.execute('''
            UPDATE TASKS set client = NULL, start_time = NULL
            WHERE (start_time IS NOT NULL) and (end_time IS NULL)''');
            server.db.commit();
            log("Clearing pending tasks from the database... done.");

        server.serve_forever();
